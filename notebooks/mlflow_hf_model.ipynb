{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "import mlflow, shutil, os\n",
    "\n",
    "class MLFlowModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"Generic deployment model using mlflow\"\"\"\n",
    "\n",
    "    def load_context(self, context: Any):\n",
    "        \"\"\"Loads necessary context.\"\"\"\n",
    "\n",
    "        import torch\n",
    "        from transformers import pipeline\n",
    "        \n",
    "        model_name = \"h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2\"\n",
    "        load_in_8bit = False\n",
    "        load_sharded = False\n",
    "        use_fast = False\n",
    "\n",
    "        model_kwargs = {}\n",
    "        \n",
    "        if load_in_8bit:\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_8bit = True,\n",
    "                llm_int8_threshold = 3.0,\n",
    "            )\n",
    "            model_kwargs[\"quantization_config\"] = quantization_config\n",
    "\n",
    "        if load_sharded:\n",
    "            device_map = \"auto\"\n",
    "        else:\n",
    "            device_map = {\"\": \"cuda:0\"}\n",
    "\n",
    "        self.pipeline = pipeline(\n",
    "            model = model_name,\n",
    "            torch_dtype = torch.float16,\n",
    "            trust_remote_code = True,\n",
    "            use_fast = use_fast,\n",
    "            device_map = device_map,\n",
    "            model_kwargs = model_kwargs\n",
    "        )\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict(self, context: Any, model_input: Dict) -> List:\n",
    "        \"\"\"Predict function\n",
    "\n",
    "        Args:\n",
    "            context: instance containing artifacts that the\n",
    "                     model can use to perform inference\n",
    "            model_input: dictionary containing input to evaluate,\n",
    "                         needs to contain 'input' key\n",
    "\n",
    "        Returns:\n",
    "            Single element list containing json encoded as string\n",
    "        \"\"\"\n",
    "\n",
    "        import json\n",
    "        from collections import defaultdict\n",
    "\n",
    "        values = defaultdict(list)\n",
    "        for idx, (k, v) in enumerate(model_input.items()):\n",
    "            for val in v.values:\n",
    "                values[k].append(val)\n",
    "\n",
    "        outputs = self.pipeline(\n",
    "            values[\"input\"],\n",
    "            min_new_tokens=2,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=False,\n",
    "            num_beams=2,\n",
    "            temperature=float(0.3),\n",
    "            repetition_penalty=float(1.2),\n",
    "            renormalize_logits=True\n",
    "        )\n",
    "        \n",
    "        d = defaultdict(list)\n",
    "        for output in outputs:\n",
    "            for k, v in output[0].items():\n",
    "                d[k].append(v)\n",
    "\n",
    "        d = json.dumps(d)\n",
    "        d = [d]\n",
    "\n",
    "        return d\n",
    "    \n",
    "def setup_mlflow(output_path: str):\n",
    "    \"\"\"Setting up mlflow ZIP archive\n",
    "\n",
    "    Args:\n",
    "        output_path: path to store output\n",
    "    \"\"\"\n",
    "\n",
    "    mlflow_model = MLFlowModel()\n",
    "\n",
    "    input_schema = mlflow.types.Schema(\n",
    "        [mlflow.types.ColSpec(name=\"input\", type=mlflow.types.DataType.string)]\n",
    "    )\n",
    "\n",
    "    output_schema = mlflow.types.Schema(\n",
    "        [mlflow.types.ColSpec(name=\"output\", type=mlflow.types.DataType.string)]\n",
    "    )\n",
    "\n",
    "    signature = mlflow.models.signature.ModelSignature(\n",
    "        inputs=input_schema, outputs=output_schema\n",
    "    )\n",
    "\n",
    "    model_path = os.path.join(output_path, \"model.mlflow\")\n",
    "    shutil.rmtree(model_path, ignore_errors=True)\n",
    "    mlflow.pyfunc.save_model(\n",
    "        path=model_path,\n",
    "        python_model=mlflow_model,\n",
    "        signature=signature,\n",
    "        pip_requirements=[\n",
    "            \"torch==2.0.0\",\n",
    "            \"transformers==4.28.1\",\n",
    "            \"accelerate==0.18.0\",\n",
    "            \"sentencepiece==0.1.96\",\n",
    "            \"bitsandbytes==0.38.1\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    _ = shutil.make_archive(model_path, \"zip\", model_path)\n",
    "\n",
    "model_path = setup_mlflow(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9d643d982847e99a0645e34e41038a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "mlflow_model = mlflow.pyfunc.load_model(\"model.mlflow\")\n",
    "\n",
    "prompts = [\n",
    "    \"How are you?\",\n",
    "    \"What is the capital of the US?\"\n",
    "]\n",
    "\n",
    "input = {\n",
    "    \"input\": prompts\n",
    "}\n",
    "\n",
    "ret = mlflow_model.predict(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing well. How about you?\n",
      "The capital of the United States of America is Washington, D.C.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "for r in json.loads(ret[0])[\"generated_text\"]:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
