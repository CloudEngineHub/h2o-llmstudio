When training MoE models like Mixtral and choosing MoECrossEntropy as loss function, this setting allows to specify the additive ratio of the load balancing loss as described in https://arxiv.org/abs/2101.03961 encouraging a balanced load across experts. 