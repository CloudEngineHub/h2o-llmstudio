"use strict";(self.webpackChunksite=self.webpackChunksite||[]).push([[211],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>p});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var s=n.createContext({}),d=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},u=function(e){var t=d(e.components);return n.createElement(s.Provider,{value:t},e.children)},h="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,i=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),h=d(a),c=o,p=h["".concat(s,".").concat(c)]||h[c]||m[c]||i;return a?n.createElement(p,r(r({ref:t},u),{},{components:a})):n.createElement(p,r({ref:t},u))}));function p(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=a.length,r=new Array(i);r[0]=c;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[h]="string"==typeof e?e:o,r[1]=l;for(var d=2;d<i;d++)r[d]=a[d];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},1150:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>d});var n=a(7462),o=(a(7294),a(3905));const i={},r="FAQs",l={unversionedId:"faqs",id:"faqs",title:"FAQs",description:"The sections below provide answers to frequently asked questions. If you have additional questions, please send them to .",source:"@site/docs/faqs.md",sourceDirName:".",slug:"/faqs",permalink:"/h2o-llmstudio/faqs",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"defaultSidebar",previous:{title:"Publish model to HuggingFace",permalink:"/h2o-llmstudio/guide/experiments/export-trained-model"}},s={},d=[{value:"How much data is generally required to fine-tune a model?",id:"how-much-data-is-generally-required-to-fine-tune-a-model",level:3},{value:"Are there any recommendations for which backbone to use? Are some backbones better for certain types of tasks?",id:"are-there-any-recommendations-for-which-backbone-to-use-are-some-backbones-better-for-certain-types-of-tasks",level:3},{value:"What if my data is not in question-and-answer form and I just have documents? How can I fine-tune the LLM model?",id:"what-if-my-data-is-not-in-question-and-answer-form-and-i-just-have-documents-how-can-i-fine-tune-the-llm-model",level:3},{value:"I encounter GPU out-of-memory issues. What can I change to be able to train large models?",id:"i-encounter-gpu-out-of-memory-issues-what-can-i-change-to-be-able-to-train-large-models",level:3},{value:"When does the model stop the fine-tuning process?",id:"when-does-the-model-stop-the-fine-tuning-process",level:3},{value:"How many records are recommended for fine-tuning?",id:"how-many-records-are-recommended-for-fine-tuning",level:3},{value:"Where does H2O LLM Studio store its data?",id:"where-does-h2o-llm-studio-store-its-data",level:3},{value:"How can I update H2O LLM Studio?",id:"how-can-i-update-h2o-llm-studio",level:3},{value:"Once I have the LoRA, what is the recommended way of utilizing it with the base model?",id:"once-i-have-the-lora-what-is-the-recommended-way-of-utilizing-it-with-the-base-model",level:3},{value:"How to use H2O LLM Studio in Windows?",id:"how-to-use-h2o-llm-studio-in-windows",level:3},{value:"How can I easily fine-tune a large language model (LLM) using the command-line interface (CLI) of H2O LLM Studio when I have limited GPU memory?",id:"how-can-i-easily-fine-tune-a-large-language-model-llm-using-the-command-line-interface-cli-of-h2o-llm-studio-when-i-have-limited-gpu-memory",level:3}],u={toc:d},h="wrapper";function m(e){let{components:t,...a}=e;return(0,o.kt)(h,(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"faqs"},"FAQs"),(0,o.kt)("p",null,"The sections below provide answers to frequently asked questions. If you have additional questions, please send them to ",(0,o.kt)("a",{parentName:"p",href:"mailto:cloud-feedback@h2o.ai"},"cloud-feedback@h2o.ai"),"."),(0,o.kt)("hr",null),(0,o.kt)("h3",{id:"how-much-data-is-generally-required-to-fine-tune-a-model"},"How much data is generally required to fine-tune a model?"),(0,o.kt)("p",null,'There is no clear answer. As a rule of thumb, 1000 to 50000 samples of conversational data should be enough. Quality and diversity is very important. Make sure to try training on a subsample of data using the "sample" parameter to see how big the impact of the dataset size is. Recent studies suggest that less data is needed for larger foundation models.'),(0,o.kt)("hr",null),(0,o.kt)("h3",{id:"are-there-any-recommendations-for-which-backbone-to-use-are-some-backbones-better-for-certain-types-of-tasks"},"Are there any recommendations for which backbone to use? Are some backbones better for certain types of tasks?"),(0,o.kt)("p",null,"The majority of the LLM backbones are trained on a very similar corpus of data. The main difference is the size of the model and the number of parameters. Usually, the larger the model, the better they are. The larger models also take longer to train. We recommend starting with the smallest model and then increasing the size if the performance is not satisfactory. If you are looking to train for tasks that are not directly question answering in English, it is also a good idea to look for specialized LLM backbones."),(0,o.kt)("hr",null),(0,o.kt)("h3",{id:"what-if-my-data-is-not-in-question-and-answer-form-and-i-just-have-documents-how-can-i-fine-tune-the-llm-model"},"What if my data is not in question-and-answer form and I just have documents? How can I fine-tune the LLM model?"),(0,o.kt)("p",null,"To train a chatbot style model, you need to convert your data into a question and answer format."),(0,o.kt)("hr",null),(0,o.kt)("h3",{id:"i-encounter-gpu-out-of-memory-issues-what-can-i-change-to-be-able-to-train-large-models"},"I encounter GPU out-of-memory issues. What can I change to be able to train large models?"),(0,o.kt)("p",null,"There are various parameters that can be tuned while keeping a specific LLM backbone fixed. It is advised to choose 4bit/8bit precision as a backbone dtype to be able to train models >=7B on a consumer type GPU. ",(0,o.kt)("a",{parentName:"p",href:"concepts#lora-low-rank-adaptation"},"LORA")," should be enabled. Besides that there are the usual parameters such as batch size and maximum sequence length that can be decreased to save GPU memory (please ensure that your prompt+answer text is not truncated too much by checking the train data insights)."),(0,o.kt)("hr",null),(0,o.kt)("h3",{id:"when-does-the-model-stop-the-fine-tuning-process"},"When does the model stop the fine-tuning process?"),(0,o.kt)("p",null,"The number of epochs are set by the user."),(0,o.kt)("hr",null),(0,o.kt)("h3",{id:"how-many-records-are-recommended-for-fine-tuning"},"How many records are recommended for fine-tuning?"),(0,o.kt)("p",null,"An order of 100K records is recommended for fine-tuning."),(0,o.kt)("hr",null),(0,o.kt)("h3",{id:"where-does-h2o-llm-studio-store-its-data"},"Where does H2O LLM Studio store its data?"),(0,o.kt)("p",null,"H2O LLM Studio stores its data in two folders located in the root directory. The folders are named ",(0,o.kt)("inlineCode",{parentName:"p"},"data")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"output"),". Here is the breakdown of the data storage structure:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"data/dbs"),": This folder contains the user database used within the app."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"data/user"),": This folder is where uploaded datasets from the user are stored."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"output/user"),": All experiments conducted in H2O LLM Studio are stored in this folder. For each experiment, a separate folder is created within the ",(0,o.kt)("inlineCode",{parentName:"li"},"output/user")," directory, which contains all the relevant data associated with that particular experiment."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"output/download"),": Utility folder that is used to store data the user downloads within the app. ")),(0,o.kt)("p",null,"It is possible to change the default folders ",(0,o.kt)("inlineCode",{parentName:"p"},"data")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"output")," in ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/h2oai/h2o-llmstudio/blob/main/app_utils/config.py"},"app_utils/config.py"),"\n(change ",(0,o.kt)("inlineCode",{parentName:"p"},"data_folder")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"output_folder"),")."),(0,o.kt)("hr",null),(0,o.kt)("h3",{id:"how-can-i-update-h2o-llm-studio"},"How can I update H2O LLM Studio?"),(0,o.kt)("p",null,"To update H2O LLM Studio, you have two options:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Using the latest main branch: Execute the commands ",(0,o.kt)("inlineCode",{parentName:"li"},"git checkout main")," and ",(0,o.kt)("inlineCode",{parentName:"li"},"git pull")," to obtain the latest updates from the main branch."),(0,o.kt)("li",{parentName:"ol"},"Using the latest release tag: Execute the commands ",(0,o.kt)("inlineCode",{parentName:"li"},"git pull")," and ",(0,o.kt)("inlineCode",{parentName:"li"},"git checkout v0.0.3")," (replace 'v0.0.3' with the desired version number) to switch to the latest release branch.")),(0,o.kt)("p",null,"The update process does not remove or erase any existing data folders or experiment records.\nThis means that all your old data, including the user database, uploaded datasets, and experiment results,\nwill still be available to you within the updated version of H2O LLM Studio."),(0,o.kt)("p",null,"Before updating, we recommend running the command ",(0,o.kt)("inlineCode",{parentName:"p"},"git rev-parse --short HEAD")," and saving the commit hash.\nThis will allow you to revert to your existing version if needed. "),(0,o.kt)("h3",{id:"once-i-have-the-lora-what-is-the-recommended-way-of-utilizing-it-with-the-base-model"},"Once I have the ",(0,o.kt)("a",{parentName:"h3",href:"/h2o-llmstudio/guide/experiments/experiment-settings#lora"},"LoRA"),", what is the recommended way of utilizing it with the base model?"),(0,o.kt)("p",null,"You can also export the LoRA weights. You may add them to the files to be exported ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/h2oai/h2o-llmstudio/blob/main/app_utils/sections/experiment.py#L1552"},"here"),". Before exporting, we merge the LoRA weights back into the original LLM backbone weights to make downstream tasks easier. You don't need to have PEFT, or anything else for your deployment."),(0,o.kt)("h3",{id:"how-to-use-h2o-llm-studio-in-windows"},"How to use H2O LLM Studio in Windows?"),(0,o.kt)("p",null,"Use WSL 2 on Windows "),(0,o.kt)("h3",{id:"how-can-i-easily-fine-tune-a-large-language-model-llm-using-the-command-line-interface-cli-of-h2o-llm-studio-when-i-have-limited-gpu-memory"},"How can I easily fine-tune a large language model (LLM) using the command-line interface (CLI) of H2O LLM Studio when I have limited GPU memory?"),(0,o.kt)("p",null,"If you have limited GPU memory but still want to fine-tune a large language model using H2O LLM Studio's CLI, there are alternative methods you can use to get started quickly."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://www.kaggle.com/code/philippsinger/h2o-llm-studio-cli/"},"Using Kaggle kernels")," "),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://colab.research.google.com/drive/1-OYccyTvmfa3r7cAquw8sioFFPJcn4R9?usp=sharing"},"Using Google Colab"))))}m.isMDXComponent=!0}}]);