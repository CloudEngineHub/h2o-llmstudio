"use strict";(self.webpackChunksite=self.webpackChunksite||[]).push([[103],{5860:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>p,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>m});var n=a(7462),r=(a(7294),a(3905)),i=a(3462);const o={},p="Create an experiment",s={unversionedId:"guide/experiments/create-an-experiment",id:"guide/experiments/create-an-experiment",title:"Create an experiment",description:"Follow the relevant steps below to create an experiment in H2O LLM Studio.",source:"@site/docs/guide/experiments/create-an-experiment.md",sourceDirName:"guide/experiments",slug:"/guide/experiments/create-an-experiment",permalink:"/h2o-llmstudio/guide/experiments/create-an-experiment",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"defaultSidebar",previous:{title:"Experiment settings",permalink:"/h2o-llmstudio/guide/experiments/experiment-settings"},next:{title:"View and manage experiments",permalink:"/h2o-llmstudio/guide/experiments/view-an-experiment"}},l={},m=[{value:"Run an experiment on the OASST data via CLI",id:"run-an-experiment-on-the-oasst-data-via-cli",level:2}],u={toc:m},d="wrapper";function h(e){let{components:t,...o}=e;return(0,r.kt)(d,(0,n.Z)({},u,o,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"create-an-experiment"},"Create an experiment"),(0,r.kt)("p",null,"Follow the relevant steps below to create an experiment in H2O LLM Studio."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"On the H2O LLM Studio left-navigation pane, click ",(0,r.kt)("strong",{parentName:"p"},"Create experiment"),". Alternatively, you can click ",(0,r.kt)("strong",{parentName:"p"},"New experiment")," on the ",(0,r.kt)(i.Z,{mdxType:"Icon"},"more_vert")," Kebab menu of the ",(0,r.kt)("a",{parentName:"p",href:"/h2o-llmstudio/guide/datasets/view-dataset"},"View datasets")," page.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Select the ",(0,r.kt)("strong",{parentName:"p"},"Dataset")," you want to use to fine-tune an LLM model.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Select the ",(0,r.kt)("strong",{parentName:"p"},"Problem type"),".")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Provide a meaningful ",(0,r.kt)("strong",{parentName:"p"},"Experiment name"),".")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Define the parameters. The most important parameters are:"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"LLM Backbone"),": This parameter determines the LLM architecture to use. It is the foundation model that you continue training. H2O LLM Studio has a predefined list of recommended types of foundation models, but you can also use ",(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/models"},"Hugging Face models"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Mask Prompt Labels"),": This option controls whether to mask the prompt labels during training and only train on the loss of the answer."),(0,r.kt)("li",{parentName:"ul"},"Hyperparameters such as ",(0,r.kt)("strong",{parentName:"li"},"Learning rate"),", ",(0,r.kt)("strong",{parentName:"li"},"Batch size"),", and number of epochs determine the training process. You can refer to the tooltips that are shown next to each hyperparameter in the GUI to learn more about them."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Evaluate Before Training"),": This option lets you evaluate the model before training, which can help you judge the quality of the LLM backbone before fine-tuning. ")),(0,r.kt)("p",{parentName:"li"},"H2O LLM Studio provides several metric options for evaluating the performance of your model. In addition to the BLEU score, H2O LLM Studio also offers the GPT3.5 and GPT4 metrics that utilize the OpenAI API to determine whether the predicted answer is more favorable than the ground truth answer. To use these metrics, you can either export your OpenAI API key as an environment variable before starting LLM Studio, or you can specify it in the ",(0,r.kt)("strong",{parentName:"p"},"Settings")," menu within the UI."),(0,r.kt)("admonition",{parentName:"li",title:"note",type:"info"},(0,r.kt)("p",{parentName:"admonition"},"H2O LLM Studio provides an overview of all the parameters you need to specify for your experiment. The default settings are suitable when you first start an experiment. To learn more about the parameters, see ",(0,r.kt)("a",{parentName:"p",href:"/h2o-llmstudio/guide/experiments/experiment-settings"},"Experiment settings"),"."))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Click ",(0,r.kt)("strong",{parentName:"p"},"Run experiment"),"."),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("img",{alt:"run-experiment",src:a(8726).Z,width:"2862",height:"1542"})))),(0,r.kt)("h2",{id:"run-an-experiment-on-the-oasst-data-via-cli"},"Run an experiment on the OASST data via CLI"),(0,r.kt)("p",null,"The steps below provide an example of how to to run an experiment on ",(0,r.kt)("a",{parentName:"p",href:"https://huggingface.co/OpenAssistant"},"OASST")," data via the command line interface (CLI)."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Get the training dataset (",(0,r.kt)("inlineCode",{parentName:"p"},"train_full.csv"),"), ",(0,r.kt)("a",{parentName:"p",href:"https://www.kaggle.com/code/philippsinger/openassistant-conversations-dataset-oasst1?scriptVersionId=126228752"},"OpenAssistant Conversations Dataset OASST1")," and place it into the ",(0,r.kt)("inlineCode",{parentName:"p"},"examples/data_oasst1")," folder; or download it directly using the ",(0,r.kt)("a",{parentName:"p",href:"https://www.kaggle.com/docs/api"},"Kaggle API")," command given below."),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kaggle kernels output philippsinger/openassistant-conversations-dataset-oasst1 -p examples/data_oasst1/\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Go into the interactive shell or open a new terminal window. Install the dependencies first, if you have not installed them already. "),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"make setup  # installs all dependencies\nmake shell\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Run the following command to run the experiment. "),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"python train.py -C examples/cfg_example_oasst1.py\n")))),(0,r.kt)("p",null,"After the experiment is completed, you can find all output artifacts in the ",(0,r.kt)("inlineCode",{parentName:"p"},"examples/output_oasst1")," folder.\nYou can then use the ",(0,r.kt)("inlineCode",{parentName:"p"},"prompt.py")," script to chat with your model."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"python prompt.py -e examples/output_oasst1\n")))}h.isMDXComponent=!0},8726:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/run-experiment-80ccf8415f026cdf51408b24ac03dc50.png"}}]);