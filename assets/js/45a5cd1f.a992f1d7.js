"use strict";(self.webpackChunksite=self.webpackChunksite||[]).push([[0],{5680:(e,t,n)=>{n.d(t,{xA:()=>c,yg:()=>h});var a=n(6540);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),d=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=d(e.components);return a.createElement(l.Provider,{value:t},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=d(n),m=r,h=p["".concat(l,".").concat(m)]||p[m]||u[m]||i;return n?a.createElement(h,o(o({ref:t},c),{},{components:n})):a.createElement(h,o({ref:t},c))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:r,o[1]=s;for(var d=2;d<i;d++)o[d]=n[d];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},3505:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var a=n(8168),r=(n(6540),n(5680));const i={description:"Learn about concepts around H2O LLM Studio."},o="Concepts",s={unversionedId:"concepts",id:"concepts",title:"Concepts",description:"Learn about concepts around H2O LLM Studio.",source:"@site/docs/concepts.md",sourceDirName:".",slug:"/concepts",permalink:"/h2o-llmstudio/concepts",draft:!1,tags:[],version:"current",frontMatter:{description:"Learn about concepts around H2O LLM Studio."},sidebar:"defaultSidebar",previous:{title:"Videos",permalink:"/h2o-llmstudio/get-started/videos"},next:{title:"Supported data connectors and format",permalink:"/h2o-llmstudio/guide/datasets/data-connectors-format"}},l={},d=[{value:"LLM",id:"llm",level:2},{value:"Parameters and hyperparameters",id:"parameters-and-hyperparameters",level:2},{value:"LLM Backbone",id:"llm-backbone",level:2},{value:"Generative AI",id:"generative-ai",level:2},{value:"Foundation model",id:"foundation-model",level:2},{value:"Fine-tuning",id:"fine-tuning",level:2},{value:"LoRA (Low-Rank Adaptation)",id:"lora-low-rank-adaptation",level:2},{value:"Quantization",id:"quantization",level:2},{value:"8-bit model training with a low memory footprint",id:"8-bit-model-training-with-a-low-memory-footprint",level:2},{value:"BLEU",id:"bleu",level:3},{value:"Perplexity",id:"perplexity",level:3}],c={toc:d},p="wrapper";function u(e){let{components:t,...n}=e;return(0,r.yg)(p,(0,a.A)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"concepts"},"Concepts"),(0,r.yg)("p",null,"H2O LLM Studio is based on a few key concepts and uses several key terms across its documentation. Each, in turn, is explained within the sections below."),(0,r.yg)("h2",{id:"llm"},"LLM"),(0,r.yg)("p",null,"A Large Language Model (LLM) is a type of AI model that uses deep learning techniques and uses massive datasets to analyze and generate human-like language. For example, many AI chatbots or AI search engines are powered by LLMs.  "),(0,r.yg)("p",null,"Generally speaking, LLMs can be characterized by the following parameters: "),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"size of the training dataset"),(0,r.yg)("li",{parentName:"ul"},"cost of training (computational power)"),(0,r.yg)("li",{parentName:"ul"},"size of the model (parameters)"),(0,r.yg)("li",{parentName:"ul"},"performance after training (or how well the model is able to respond to a particular question)")),(0,r.yg)("h2",{id:"parameters-and-hyperparameters"},"Parameters and hyperparameters"),(0,r.yg)("p",null,"In the context of an LLM, parameters and hyperparameters are a crucial part of determinining the model's performance and overall behaviour. "),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Parameters:")," The internal variables of the model that are learned during the training process. In the case of an LLM, parameters typically include the weights and biases associated with the neural network layers. The values of parameters directly influence the model's predictions and the quality of generated text.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Hyperparameters:")," The configuration choices that are set before training the model and are not learned directly from the data (e.g., number of epochs, batch size etc.). These choices impact the learning process and influence the model's overall behavior. Hyperparameters need to be tuned and optimized to achieve the best performance. H2O LLM Studio GUI shows tooltips next to each hyperparameter to explain what each hyperparameter is for. You can also see the following references for more details about hyperparameters in H2O LLM Studio."),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},"Dataset settings"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"./guide/experiments/experiment-settings"},"Experiment settings"))))),(0,r.yg)("h2",{id:"llm-backbone"},"LLM Backbone"),(0,r.yg)("p",null,"LLM Backbone is a key hyperparamter that determines the model's architecture. This option is the most important setting when it comes to experiment creation, as it sets the pretrained model weights. For more information about LLM Backbone, see ",(0,r.yg)("a",{parentName:"p",href:"/h2o-llmstudio/guide/experiments/experiment-settings#llm-backbone"},"Experiment settings"),"."),(0,r.yg)("h2",{id:"generative-ai"},"Generative AI"),(0,r.yg)("p",null,"Generative AI refers to AI models that can generate new content, such as images, videos, or text, that did not exist before. These models learn from large datasets and use this knowledge to create new content that is similar in style or content to the original dataset."),(0,r.yg)("h2",{id:"foundation-model"},"Foundation model"),(0,r.yg)("p",null,"A particular adaptive model that has been trained on a large amount of data and starts to derive relationships between words and concepts. Foundation models are fine-tuned to become more specific and adapt to the related domain more efficiently."),(0,r.yg)("h2",{id:"fine-tuning"},"Fine-tuning"),(0,r.yg)("p",null,"Fine-tuning refers to the process of taking a pre-trained language model and further training it on a specific task or domain to improve its performance on that task. It is an important technique used to adapt LLMs to specific tasks and domains. "),(0,r.yg)("h2",{id:"lora-low-rank-adaptation"},"LoRA (Low-Rank Adaptation)"),(0,r.yg)("p",null,"Low-Rank Adapation (LoRa) involves modifying the pre-trained model by adjusting its weights and biases to better fit the new task. This adaptation is done in a way that preserves the pre-trained weights from the original dataset while also adjusting for the new task's specific requirements. This method of training or fine-turning models consumes less memory. By using low rank adaptation, the pre-trained model can be quickly adapted to new tasks, without requiring a large amount of new training data."),(0,r.yg)("h2",{id:"quantization"},"Quantization"),(0,r.yg)("p",null,"Quantization is a technique used to reduce the size and memory requirements of a large language model without sacrificing its accuracy. This is done by converting the floating-point numbers used to represent the model's parameters to lower-precision numbers, such as half-floats or bfloat16. Quantization can be used to make language models more accessible to users with limited computing resources. "),(0,r.yg)("h2",{id:"8-bit-model-training-with-a-low-memory-footprint"},"8-bit model training with a low memory footprint"),(0,r.yg)("p",null,"8-bit model training with a low memory footprint refers to a fine-tuning technique that reduces the memory requirements for training neural networks by using 8-bit integers instead of 32-bit floating-point numbers. This approach can significantly reduce the amount of memory needed to store the model's parameters and can make it possible to train larger models on hardware with limited memory capacity."),(0,r.yg)("h3",{id:"bleu"},"BLEU"),(0,r.yg)("p",null,"Bilingual Evaluation Understudy (BLEU) is a model evaluation metric that is used to measure the quality of the predicted text against the input text. "),(0,r.yg)("p",null,"BLEU: BLEU (Bilingual Evaluation Understudy) measures the quality of machine-generated texts by comparing them to reference texts by calculating a score between 0 and 1, where a higher score indicates a better match with the reference text.  BLEU is based on the concept of n-grams, which are contiguous sequences of words. The different variations of BLEU such as BLEU-1, BLEU-2, BLEU-3, and BLEU-4 differ in the size of the n-grams considered for evaluation.  BLEU-n measures the precision of n-grams (n consecutive words) in the generated text compared to the reference text. It calculates the precision score by counting the number of overlapping n-grams and dividing it by the total number of n-grams in the generated text."),(0,r.yg)("h3",{id:"perplexity"},"Perplexity"),(0,r.yg)("p",null,"Perplexity (PPL) is a commonly used evaluation metric. It measures the confidence a model has in its predictions, or in simpler words how 'perplexed' or surprised it is by seeing new data. Perplexity is defined as the exponentiated cross-entropy of a sequence of tokens. Lower perplexity means the model is highly confident and accurate in the sequence of tokens it is responding with."))}u.isMDXComponent=!0}}]);